{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "collapsed_sections": [
        "EJ41JBWqlcFO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "sscBf7v8dxS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny-hblGWVx86",
        "outputId": "9e2c2e8c-b6ba-4a48-8320-f27a4eb829cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.6/620.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for adapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.20.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install adapt pyswarm tensorflow==2.15.1 --quiet\n",
        "!pip install adapt pyswarm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import wasserstein_distance\n",
        "from scipy.special import expit, logit\n",
        "\n",
        "from pyswarm import pso\n",
        "from adapt.feature_based import DANN, ADDA\n",
        "from adapt.utils import make_classification_da\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "pGUym3nK7PCr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE_FOR_SKLEARN = check_random_state(42)\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if tf.config.list_physical_devices(\"GPU\") else \"cpu\""
      ],
      "metadata": {
        "id": "ctP73UXW7VKO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  tf.random.set_seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "  os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      tf.config.experimental.set_deterministic(True)\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "      print(e)"
      ],
      "metadata": {
        "id": "9ot0nrqS7Y1Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed()"
      ],
      "metadata": {
        "id": "3vH7p-UZ7dBT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Datasets"
      ],
      "metadata": {
        "id": "QiXMdfT-3QNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_base_link = \"http://cicresearch.ca/IOTDataset/CIC-BCCC-NRC-TabularIoTAttacks-2024/Dataset/CIC-BCCC-NRC-ACI-IOT-2023/\"\n",
        "target_base_link = \"http://cicresearch.ca/IOTDataset/CIC-BCCC-NRC-TabularIoTAttacks-2024/Dataset/CIC-BCCC-NRC-IoMT-2024/\"\n",
        "data_files = [\"DoS%20ICMP%20Flood.csv\", \"DoS%20UDP%20Flood.csv\", \"Recon%20OS%20Scan.csv\", \"Benign%20Traffic.csv\", \"MITM%20ARP%20Spoofing.csv\", \"Recon%20Ping%20Sweep.csv\", \"Recon%20Vulnerability%20Scan.csv\"]"
      ],
      "metadata": {
        "id": "V5R3bCqS2--3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_dfs = [pd.read_csv(source_base_link + file) for file in data_files]\n",
        "target_dfs = [pd.read_csv(target_base_link + file) for file in data_files]"
      ],
      "metadata": {
        "id": "_WDlXdXL3T3o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = pd.concat(source_dfs, ignore_index=True) # ACI-IOT-2023\n",
        "target = pd.concat(target_dfs, ignore_index=True) # IoMT-2024\n",
        "print(source.shape)\n",
        "print(target.shape)\n",
        "# print(source.columns)\n",
        "# print(target.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqBd5eXb3ad3",
        "outputId": "3f182a39-5bbb-4a2a-801d-fe064ecfa791"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(233331, 85)\n",
            "(132604, 85)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(source[\"Attack Name\"].value_counts())\n",
        "print(target[\"Attack Name\"].value_counts())\n",
        "print(source[\"Label\"].value_counts())\n",
        "print(target[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQbZXOHS3bno",
        "outputId": "fd38b690-7a1a-4bb1-e76b-8c3947afc057"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack Name\n",
            "Benign Traffic              86525\n",
            "Recon Ping Sweep            47123\n",
            "Recon OS Scan               42173\n",
            "Recon Vulnerability Scan    39489\n",
            "MITM ARP Spoofing           14768\n",
            "DoS UDP Flood                1848\n",
            "DoS ICMP Flood               1405\n",
            "Name: count, dtype: int64\n",
            "Attack Name\n",
            "Recon OS Scan               85317\n",
            "Benign Traffic              32620\n",
            "Recon Vulnerability Scan     8321\n",
            "DoS UDP Flood                3115\n",
            "DoS ICMP Flood               2107\n",
            "MITM ARP Spoofing            1053\n",
            "Recon Ping Sweep               71\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "1    146806\n",
            "0     86525\n",
            "Name: count, dtype: int64\n",
            "Label\n",
            "1    99984\n",
            "0    32620\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "useful_columns = [\n",
        "    'Flow Duration', 'Total Fwd Packet', 'Total Bwd packets',\n",
        "    'Total Length of Fwd Packet', 'Total Length of Bwd Packet',\n",
        "    'Fwd Packet Length Max', 'Fwd Packet Length Min',\n",
        "    'Fwd Packet Length Mean', 'Fwd Packet Length Std',\n",
        "    'Bwd Packet Length Max', 'Bwd Packet Length Min',\n",
        "    'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "    'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean',\n",
        "    'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
        "    'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
        "    'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total',\n",
        "    'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "    'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
        "    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
        "    'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
        "    'Packet Length Min', 'Packet Length Max',\n",
        "    'Packet Length Mean', 'Packet Length Std',\n",
        "    'Packet Length Variance', 'FIN Flag Count',\n",
        "    'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count',\n",
        "    'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count',\n",
        "    'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size',\n",
        "    'Fwd Segment Size Avg', 'Bwd Segment Size Avg',\n",
        "    'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg',\n",
        "    'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg',\n",
        "    'Bwd Packet/Bulk Avg', 'Bwd Bulk Rate Avg',\n",
        "    'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "    'Subflow Bwd Packets', 'Subflow Bwd Bytes',\n",
        "    'FWD Init Win Bytes', 'Bwd Init Win Bytes',\n",
        "    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean',\n",
        "    'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "    'Idle Std', 'Idle Max', 'Idle Min', 'Protocol', 'Label'\n",
        "]"
      ],
      "metadata": {
        "id": "3porEhAi36Tu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = source[useful_columns]\n",
        "target = target[useful_columns]\n",
        "X_source, y_source, X_target, y_target = source.drop(columns=['Label']), source['Label'], target.drop(columns=['Label']), target['Label']"
      ],
      "metadata": {
        "id": "5z9nR7fy3-s3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify feature types\n",
        "categorical_features = ['Protocol']\n",
        "numerical_features = [f for f in useful_columns[:-1] if f != 'Protocol']\n",
        "\n",
        "# Create preprocessing pipeline\n",
        "# The sparse argument has been changed to sparse_output\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', StandardScaler(), numerical_features),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "])\n",
        "\n",
        "X_source = preprocessor.fit_transform(X_source)\n",
        "X_target = preprocessor.transform(X_target)"
      ],
      "metadata": {
        "id": "e4hXVHf44CCS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation Functions"
      ],
      "metadata": {
        "id": "S_Aam5xZ4lQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wasserstein"
      ],
      "metadata": {
        "id": "RaGu6ut34o8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wasserstein_transform(Xs, Xt):\n",
        "    \"\"\"\n",
        "    Perform Wasserstein Distance-based alignment on source features.\n",
        "\n",
        "    Parameters:\n",
        "    Xs: np.array, shape (ns, d)\n",
        "        Source feature matrix (samples x features)\n",
        "    Xt: np.array, shape (nt, d)\n",
        "        Target feature matrix (samples x features)\n",
        "\n",
        "    Returns:\n",
        "    Xs_aligned: np.array, shape (ns, d)\n",
        "        Wasserstein-aligned source features\n",
        "    \"\"\"\n",
        "    Xs_mean, Xt_mean = np.mean(Xs, axis=0), np.mean(Xt, axis=0)\n",
        "    Xs_std, Xt_std = np.std(Xs, axis=0), np.std(Xt, axis=0)\n",
        "\n",
        "    # Compute original Wasserstein distance before transformation\n",
        "    original_distances = [wasserstein_distance(Xs[:, i], Xt[:, i]) for i in range(Xs.shape[1])]\n",
        "    original_avg_distance = np.mean(original_distances)\n",
        "\n",
        "    # Align mean and standard deviation\n",
        "    Xs_aligned = (Xs - Xs_mean) * (Xt_std / (Xs_std + 1e-6)) + Xt_mean\n",
        "\n",
        "    # Compute new Wasserstein distance after transformation\n",
        "    new_distances = [wasserstein_distance(Xs_aligned[:, i], Xt[:, i]) for i in range(Xs.shape[1])]\n",
        "    new_avg_distance = np.mean(new_distances)\n",
        "\n",
        "    print(f\"Original Average Wasserstein Distance: {original_avg_distance:.6f}\")\n",
        "    print(f\"New Average Wasserstein Distance After Transformation: {new_avg_distance:.6f}\")\n",
        "\n",
        "    return Xs_aligned"
      ],
      "metadata": {
        "id": "V_YOSKr94YFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_source_aligned_wasserstein = wasserstein_transform(X_source, X_target)\n",
        "# higher_order_statisctics(X_source_aligned_wasserstein, X_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2mcKu2M5sCB",
        "outputId": "15172a6d-52aa-4fac-d934-d1a0a8e472d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Average Wasserstein Distance: 0.255301\n",
            "New Average Wasserstein Distance After Transformation: 0.230939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wasserstein Classwise"
      ],
      "metadata": {
        "id": "ENVojFYD51q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wasserstein_transform_classwise(Xs, Xt, ys, yt):\n",
        "    \"\"\"\n",
        "    Perform class-wise Wasserstein Distance-based alignment on source features.\n",
        "\n",
        "    Parameters:\n",
        "    Xs: np.array, shape (ns, d)\n",
        "        Source feature matrix (samples x features)\n",
        "    Xt: np.array, shape (nt, d)\n",
        "        Target feature matrix (samples x features)\n",
        "    ys: np.array, shape (ns,)\n",
        "        Source labels\n",
        "    yt: np.array, shape (nt,)\n",
        "        Target labels\n",
        "\n",
        "    Returns:\n",
        "    Xs_aligned: np.array, shape (ns, d)\n",
        "        Wasserstein-aligned source features\n",
        "    \"\"\"\n",
        "    Xs_aligned = np.zeros_like(Xs)\n",
        "\n",
        "    # Unique class labels\n",
        "    unique_classes_s = np.unique(ys)\n",
        "    unique_classes_t = np.unique(yt)\n",
        "\n",
        "    original_distances = []\n",
        "    new_distances = []\n",
        "\n",
        "    for cls in unique_classes_s:\n",
        "        # Get the samples belonging to the current class in source and target\n",
        "        Xs_cls = Xs[ys == cls]\n",
        "        Xt_cls = Xt[yt == cls]\n",
        "\n",
        "        if len(Xt_cls) == 0:\n",
        "            continue\n",
        "\n",
        "        # Compute original Wasserstein distance for this class\n",
        "        original_class_distances = [wasserstein_distance(Xs_cls[:, i], Xt_cls[:, i]) for i in range(Xs.shape[1])]\n",
        "        original_avg_class_distance = np.mean(original_class_distances)\n",
        "        original_distances.append(original_avg_class_distance)\n",
        "\n",
        "        # Compute the mean and std for each class in source and target\n",
        "        Xs_mean, Xt_mean = np.mean(Xs_cls, axis=0), np.mean(Xt_cls, axis=0)\n",
        "        Xs_std, Xt_std = np.std(Xs_cls, axis=0), np.std(Xt_cls, axis=0)\n",
        "\n",
        "        # Apply Wasserstein transformation per class\n",
        "        Xs_aligned[ys == cls] = (Xs_cls - Xs_mean) * (Xt_std / (Xs_std + 1e-6)) + Xt_mean\n",
        "\n",
        "        # Compute new Wasserstein distance for this class\n",
        "        new_class_distances = [wasserstein_distance(Xs_aligned[ys == cls][:, i], Xt_cls[:, i]) for i in range(Xs.shape[1])]\n",
        "        new_avg_class_distance = np.mean(new_class_distances)\n",
        "        new_distances.append(new_avg_class_distance)\n",
        "\n",
        "        print(f\"Class {cls}: Original Avg Wasserstein Distance = {original_avg_class_distance:.6f}, New Avg Wasserstein Distance = {new_avg_class_distance:.6f}\")\n",
        "\n",
        "    # Compute overall Wasserstein distances\n",
        "    overall_original_distance = np.mean(original_distances) if original_distances else 0.0\n",
        "    overall_new_distance = np.mean(new_distances) if new_distances else 0.0\n",
        "\n",
        "    print(f\"\\nOverall Original Avg Wasserstein Distance: {overall_original_distance:.6f}\")\n",
        "    print(f\"Overall New Avg Wasserstein Distance After Transformation: {overall_new_distance:.6f}\")\n",
        "\n",
        "    return Xs_aligned\n"
      ],
      "metadata": {
        "id": "opihKG1n5ymw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_source_aligned_wasserstein_classwise = wasserstein_transform_classwise(X_source, X_target, y_source, y_target)\n",
        "# higher_order_statisctics(X_source_aligned_wasserstein_classwise, X_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eabx3oPr7Rm4",
        "outputId": "f0b31c45-ae29-4f3f-e5b4-eeba03b90914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: Original Avg Wasserstein Distance = 0.805725, New Avg Wasserstein Distance = 0.654817\n",
            "Class 1: Original Avg Wasserstein Distance = 0.095517, New Avg Wasserstein Distance = 0.080339\n",
            "\n",
            "Overall Original Avg Wasserstein Distance: 0.450621\n",
            "Overall New Avg Wasserstein Distance After Transformation: 0.367578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain-Adversarial Neural Networks (DANN)"
      ],
      "metadata": {
        "id": "M9s70LE_jqi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.autograd import Function\n",
        "import numpy as np\n",
        "\n",
        "# Gradient Reversal Layer\n",
        "class GradReverse(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd):\n",
        "        ctx.lambd = lambd\n",
        "        return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambd, None\n",
        "\n",
        "def grad_reverse(x, lambd=1.0):\n",
        "    return GradReverse.apply(x, lambd)\n",
        "\n",
        "# Modified DANN (outputs same dimension as input)\n",
        "class DANN_SameDim(nn.Module):\n",
        "    def __init__(self, input_dim, class_num=2):\n",
        "        super(DANN_SameDim, self).__init__()\n",
        "\n",
        "        # Feature extractor with same input/output dim\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "        )\n",
        "\n",
        "        # Label predictor\n",
        "        self.class_classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, class_num),\n",
        "        )\n",
        "\n",
        "        # Domain classifier\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, lambd=1.0):\n",
        "        feat = self.feature(x)\n",
        "        class_out = self.class_classifier(feat)\n",
        "        reverse_feat = grad_reverse(feat, lambd)\n",
        "        domain_out = self.domain_classifier(reverse_feat)\n",
        "        return class_out, domain_out\n",
        "\n",
        "# Final transformation function\n",
        "def dann_transform_same_dim(Xs, ys, Xt, epochs=20, batch_size=64, lr=1e-3, device='cpu'):\n",
        "    \"\"\"\n",
        "    Perform DANN-based alignment while preserving original feature dimensions.\n",
        "\n",
        "    Parameters:\n",
        "    - Xs: np.array (ns, d)\n",
        "    - ys: np.array (ns,)\n",
        "    - Xt: np.array (nt, d)\n",
        "\n",
        "    Returns:\n",
        "    - Xs_aligned: np.array (ns, d) - same shape as Xs\n",
        "    \"\"\"\n",
        "    # Convert to tensors\n",
        "    Xs_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
        "    ys_tensor = torch.tensor(ys, dtype=torch.long)\n",
        "    Xt_tensor = torch.tensor(Xt, dtype=torch.float32)\n",
        "\n",
        "    source_dataset = TensorDataset(Xs_tensor, ys_tensor)\n",
        "    target_dataset = TensorDataset(Xt_tensor, torch.zeros(len(Xt_tensor)))  # dummy labels\n",
        "\n",
        "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
        "    target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    input_dim = Xs.shape[1]\n",
        "    class_num = len(np.unique(ys))\n",
        "\n",
        "    model = DANN_SameDim(input_dim=input_dim, class_num=class_num).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "    criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "    # === Training ===\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        len_dataloader = min(len(source_loader), len(target_loader))\n",
        "        data_zip = zip(source_loader, target_loader)\n",
        "\n",
        "        for i, ((source_data, source_label), (target_data, _)) in enumerate(data_zip):\n",
        "            if i >= len_dataloader:\n",
        "                break\n",
        "\n",
        "            source_data, source_label = source_data.to(device), source_label.to(device)\n",
        "            target_data = target_data.to(device)\n",
        "\n",
        "            combined_data = torch.cat([source_data, target_data], dim=0)\n",
        "            domain_label = torch.cat([\n",
        "                torch.zeros(source_data.size(0)).long(),\n",
        "                torch.ones(target_data.size(0)).long()\n",
        "            ]).to(device)\n",
        "\n",
        "            # Grad reversal schedule\n",
        "            p = float(i + epoch * len_dataloader) / (epochs * len_dataloader)\n",
        "            lambd = 2. / (1. + np.exp(-10 * p)) - 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            class_out, domain_out = model(combined_data, lambd=lambd)\n",
        "\n",
        "            class_loss = criterion_class(class_out[:source_data.size(0)], source_label)\n",
        "            domain_loss = criterion_domain(domain_out, domain_label)\n",
        "            loss = class_loss + domain_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # === Feature Extraction ===\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Xs_tensor = Xs_tensor.to(device)\n",
        "        aligned_features = model.feature(Xs_tensor).cpu().numpy()\n",
        "\n",
        "    return aligned_features\n"
      ],
      "metadata": {
        "id": "1-lHLQW7jupE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_source_aligned_dann = dann_transform_same_dim(\n",
        "    Xs=X_source,\n",
        "    ys=y_source,\n",
        "    Xt=X_target,\n",
        "    epochs=30,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "print(\"Shape match check:\", X_source.shape, X_source_aligned_dann.shape, X_target.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W6I6KBCs_hV",
        "outputId": "0558b259-e548-4fb7-f1ee-28c1d8234043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape match check: (233331, 77) (233331, 77) (132604, 77)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adapt DANN"
      ],
      "metadata": {
        "id": "YQdr93GhuJZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y --quiet\n",
        "!apt-get install python3.9 python3.9-distutils --quiet # Replace 3.10 with your desired version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UKBHnMXxtkf4",
        "outputId": "14a868f9-af8c-4758-fa4c-48fedfd014e5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python3.9 is already the newest version (3.9.23-1+jammy1).\n",
            "python3.9-distutils is already the newest version (3.9.23-1+jammy1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 147 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "id": "GdwVCUjzyiYj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "eb1309bc-a35c-42f0-8368-9a1320f3f847"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.14.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.14.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Using cached tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Using cached keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.1)\n",
            "Using cached tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "Using cached keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Using cached protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Using cached tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: protobuf, ml-dtypes, keras, tensorboard, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 6.32.1\n",
            "    Uninstalling protobuf-6.32.1:\n",
            "      Successfully uninstalled protobuf-6.32.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml_dtypes 0.5.3\n",
            "    Uninstalling ml_dtypes-0.5.3:\n",
            "      Successfully uninstalled ml_dtypes-0.5.3\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.11.3\n",
            "    Uninstalling keras-3.11.3:\n",
            "      Successfully uninstalled keras-3.11.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.20.0\n",
            "    Uninstalling tensorboard-2.20.0:\n",
            "      Successfully uninstalled tensorboard-2.20.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.20.0\n",
            "    Uninstalling tensorflow-2.20.0:\n",
            "      Successfully uninstalled tensorflow-2.20.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikeras 0.13.0 requires keras>=3.2.0, but you have keras 2.15.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 protobuf-4.25.8 tensorboard-2.15.2 tensorflow-2.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "keras",
                  "ml_dtypes",
                  "tensorflow"
                ]
              },
              "id": "ea211be809b54dd28eff11a12e7b74da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from adapt.utils import make_classification_da\n",
        "from adapt.feature_based import DANN\n",
        "import numpy as np\n",
        "\n",
        "y_source_np = np.array(y_source)\n",
        "adaptive_dann = DANN() # Initialize CORAL with regularization (lambda_=0.5)\n",
        "adaptive_dann.fit(X_source, y_source_np, Xt=X_target) # Fit CORAL to source and target data, explicitly providing y_source\n",
        "# adaptive_dann.fit(X=X_source, Xt=X_target) # Fit CORAL to source and target data, explicitly providing y_source\n",
        "X_source_aligned_dann = adaptive_dann.transform(X_source)\n",
        "X_target_aligned_dann = adaptive_dann.transform(X_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvYWMrt9gVH5",
        "outputId": "bc83b7b9-4cbc-4270-edef-8a162bf82e1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7292/7292 [==============================] - 14s 2ms/step - loss: 0.1024 - disc_loss: 1.3425\n",
            "7292/7292 [==============================] - 7s 894us/step\n",
            "4144/4144 [==============================] - 4s 855us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adversarial Discriminative Domain Adaptation (ADDA)"
      ],
      "metadata": {
        "id": "EJ41JBWqlcFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 2)  # domain: 0 = source, 1 = target\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Feature extractor that preserves input dimensions (same as DANN_SameDim)\n",
        "class FeatureExtractorSameDim(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(FeatureExtractorSameDim, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Label classifier (used only in source training phase)\n",
        "class LabelClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, class_num):\n",
        "        super(LabelClassifier, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, class_num)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "E8akDTCdllDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adda_transform_same_dim(Xs, ys, Xt, epochs_src=20, epochs_adv=30, batch_size=64, lr=1e-3, device='cpu'):\n",
        "    \"\"\"\n",
        "    Perform ADDA-based alignment of source features to target distribution, keeping original feature dimensions.\n",
        "\n",
        "    Returns:\n",
        "    - Xs_aligned_adda: source features aligned to target domain\n",
        "    \"\"\"\n",
        "\n",
        "    input_dim = Xs.shape[1]\n",
        "    class_num = len(np.unique(ys))\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    Xs_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
        "    ys_tensor = torch.tensor(ys, dtype=torch.long)\n",
        "    Xt_tensor = torch.tensor(Xt, dtype=torch.float32)\n",
        "\n",
        "    source_dataset = TensorDataset(Xs_tensor, ys_tensor)\n",
        "    target_dataset = TensorDataset(Xt_tensor, torch.zeros(len(Xt_tensor)))  # dummy\n",
        "\n",
        "    source_loader = DataLoader(source_dataset, batch_size=batch_size, shuffle=True)\n",
        "    target_loader = DataLoader(target_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # === Stage 1: Train source encoder + classifier ===\n",
        "    source_encoder = FeatureExtractorSameDim(input_dim).to(device)\n",
        "    label_classifier = LabelClassifier(input_dim, class_num).to(device)\n",
        "\n",
        "    optimizer_src = optim.Adam(list(source_encoder.parameters()) + list(label_classifier.parameters()), lr=lr)\n",
        "    criterion_class = nn.CrossEntropyLoss()\n",
        "\n",
        "    source_encoder.train()\n",
        "    label_classifier.train()\n",
        "\n",
        "    for epoch in range(epochs_src):\n",
        "        for xb, yb in source_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "            optimizer_src.zero_grad()\n",
        "            features = source_encoder(xb)\n",
        "            preds = label_classifier(features)\n",
        "            loss = criterion_class(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer_src.step()\n",
        "\n",
        "    # === Stage 2: Adversarial training ===\n",
        "    target_encoder = FeatureExtractorSameDim(input_dim).to(device)\n",
        "    discriminator = Discriminator(input_dim).to(device)\n",
        "\n",
        "    optimizer_tgt = optim.Adam(target_encoder.parameters(), lr=lr)\n",
        "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "    criterion_domain = nn.CrossEntropyLoss()\n",
        "\n",
        "    target_encoder.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    for epoch in range(epochs_adv):\n",
        "        len_loader = min(len(source_loader), len(target_loader))\n",
        "        zip_loader = zip(source_loader, target_loader)\n",
        "\n",
        "        for (src_batch, _), (tgt_batch, _) in zip_loader:\n",
        "            src_data = src_batch.to(device)\n",
        "            tgt_data = tgt_batch.to(device)\n",
        "\n",
        "            # === 1. Train discriminator ===\n",
        "            source_feat = source_encoder(src_data).detach()\n",
        "            target_feat = target_encoder(tgt_data).detach()\n",
        "\n",
        "            domain_input = torch.cat([source_feat, target_feat], dim=0)\n",
        "            domain_labels = torch.cat([\n",
        "                torch.zeros(source_feat.size(0)).long(),\n",
        "                torch.ones(target_feat.size(0)).long()\n",
        "            ]).to(device)\n",
        "\n",
        "            optimizer_d.zero_grad()\n",
        "            domain_preds = discriminator(domain_input)\n",
        "            loss_d = criterion_domain(domain_preds, domain_labels)\n",
        "            loss_d.backward()\n",
        "            optimizer_d.step()\n",
        "\n",
        "            # === 2. Train target encoder to fool discriminator ===\n",
        "            target_feat = target_encoder(tgt_data)\n",
        "            domain_preds_tgt = discriminator(target_feat)\n",
        "            fool_labels = torch.zeros(target_feat.size(0)).long().to(device)\n",
        "\n",
        "            optimizer_tgt.zero_grad()\n",
        "            loss_tgt = criterion_domain(domain_preds_tgt, fool_labels)\n",
        "            loss_tgt.backward()\n",
        "            optimizer_tgt.step()\n",
        "\n",
        "    # === Output transformed source features ===\n",
        "    source_encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        Xs_tensor = Xs_tensor.to(device)\n",
        "        Xs_aligned = source_encoder(Xs_tensor).cpu().numpy()\n",
        "\n",
        "    return Xs_aligned\n"
      ],
      "metadata": {
        "id": "VRSKOJ9y_3TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_source_aligned_adda = adda_transform_same_dim(\n",
        "    Xs=X_source,\n",
        "    ys=y_source,\n",
        "    Xt=X_target,\n",
        "    epochs_src=20,\n",
        "    epochs_adv=30,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "print(\"Shape check:\", X_source.shape, X_source_aligned_adda.shape, X_target.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9b_nzjq_58x",
        "outputId": "bee5279a-d76c-4e59-bbfd-7d56c81118e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape check: (233331, 77) (233331, 77) (132604, 77)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adapt ADDA"
      ],
      "metadata": {
        "id": "38HkJ533u6z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_source_np = np.array(y_source)\n",
        "adaptive_adda = ADDA(random_state=42) # Initialize CORAL with regularization (lambda_=0.5)\n",
        "adaptive_adda.fit(X_source, y_source_np, Xt=X_target) # Fit CORAL to source and target data, explicitly providing y_source\n",
        "X_source_aligned_adda = adaptive_adda.transform(X_source)\n",
        "X_target_aligned_adda = adaptive_adda.transform(X_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vRfFeg12Sqq",
        "outputId": "0da6194c-ae80-4bda-9cc6-1f4d3540637e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7292/7292 [==============================] - 17s 2ms/step - loss: 0.0943\n",
            "7292/7292 [==============================] - 13s 2ms/step - disc_loss: 1.3520\n",
            "7292/7292 [==============================] - 7s 893us/step\n",
            "4144/4144 [==============================] - 4s 990us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Models"
      ],
      "metadata": {
        "id": "edKHMAJ89S14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "S6lCctCXGHvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Without Domain Adaptation"
      ],
      "metadata": {
        "id": "J6YgKqXw9U12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Logistic Regression without domain adaptation\n",
        "# #############################################\n",
        "\n",
        "\n",
        "# Initialize Logistic Regression\n",
        "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
        "\n",
        "# Before CORAL\n",
        "train_start_time = time.time()\n",
        "lr.fit(X_source, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_before = lr.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "train_time_before = time.time() - train_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "score_time_before = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "print(f\"Normal Approach: Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal Approach):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():  # Check if it's a class label\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPScuAmw7Xcl",
        "outputId": "f9513e9f-ddf0-4025-81f4-2a3cc64b686d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Normal Approach: Accuracy = 0.8192, F1 = 0.7903, Precision = 0.8185, Recall = 0.8192\n",
            "Training Time: 17.7447 seconds\n",
            "Testing Time: 0.1876 seconds\n",
            "Scoring Time: 0.1690 seconds\n",
            "Training Time Without Testing And Score Calulations: 17.3881 seconds\n",
            "Testing Time Without Score Calulations: 0.0186 seconds\n",
            "Class-wise Metrics (Normal Approach):\n",
            "Label 0: Precision = 0.8144, Recall = 0.3434, F1-score = 0.4831\n",
            "Label 1: Precision = 0.8198, Recall = 0.9745, F1-score = 0.8905\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[11202 21418]\n",
            " [ 2553 97431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Wasserstein"
      ],
      "metadata": {
        "id": "3TDTAQR2ENLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Logistic Regression Wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
        "\n",
        "train_start_time = time.time()\n",
        "lr.fit(X_source_aligned_wasserstein, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = lr.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Wasserstein:\")\n",
        "print(f\" Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7bV8v_8CEP9",
        "outputId": "ea38b281-7873-4620-b804-1eb7e22b2445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Wasserstein:\n",
            " Wasserstein:  Accuracy = 0.9138, F1 = 0.9134, Precision = 0.9131, Recall = 0.9138\n",
            "Training Time: 71.5103 seconds\n",
            "Testing Time: 0.1022 seconds\n",
            "Scoring Time: 0.0834 seconds\n",
            "Training Time Without Testing And Score Calulations: 71.3248 seconds\n",
            "Testing Time Without Score Calulations: 0.0189 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8349, Recall = 0.8099, F1-score = 0.8222\n",
            "Label 1: Precision = 0.9386, Recall = 0.9477, F1-score = 0.9431\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[26418  6202]\n",
            " [ 5225 94759]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-7FWEFzFB4R",
        "outputId": "1472f8b1-5f95-46f1-f4aa-6374451add07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0946, F1 Δ = +0.1231, Precision Δ = +0.0946, Recall Δ = +0.0946\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0205\n",
            "  Recall Δ = +0.4665\n",
            "  F1-score Δ = +0.3391\n",
            "Label 1:\n",
            "  Precision Δ = +0.1188\n",
            "  Recall Δ = -0.0267\n",
            "  F1-score Δ = +0.0527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Classwise Wasserstein"
      ],
      "metadata": {
        "id": "U3UPjBZfGvBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Logistic Regression Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
        "train_start_time = time.time()\n",
        "lr.fit(X_source_aligned_wasserstein_classwise, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = lr.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Logistic Regression Classwise Wasserstein:\")\n",
        "print(f\"Classwise Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Classwise Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Classwise Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igbCCrWrG1tO",
        "outputId": "34eb3d99-8c98-487b-f548-18fb75e98da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Classwise Wasserstein:\n",
            "Classwise Wasserstein:  Accuracy = 0.9404, F1 = 0.9417, Precision = 0.9459, Recall = 0.9404\n",
            "Training Time: 6.7376 seconds\n",
            "Testing Time: 0.0852 seconds\n",
            "Scoring Time: 0.0721 seconds\n",
            "Training Time Without Testing And Score Calulations: 6.5803 seconds\n",
            "Testing Time Without Score Calulations: 0.0132 seconds\n",
            "Class-wise Metrics (Classwise Wasserstein):\n",
            "Label 0: Precision = 0.8308, Recall = 0.9517, F1-score = 0.8872\n",
            "Label 1: Precision = 0.9835, Recall = 0.9368, F1-score = 0.9595\n",
            "\n",
            "Confusion Matrix (Classwise Wasserstein):\n",
            "[[31045  1575]\n",
            " [ 6322 93662]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA4ck9DiG8zy",
        "outputId": "1389ba43-2f73-4b7a-a40d-4abc5ed8df56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.1212, F1 Δ = +0.1515, Precision Δ = +0.1274, Recall Δ = +0.1212\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0164\n",
            "  Recall Δ = +0.6083\n",
            "  F1-score Δ = +0.4041\n",
            "Label 1:\n",
            "  Precision Δ = +0.1637\n",
            "  Recall Δ = -0.0377\n",
            "  F1-score Δ = +0.0691\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression DANN"
      ],
      "metadata": {
        "id": "kxS0h8GXj_At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Logistic Regression DANN\n",
        "#############################################\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
        "train_start_time = time.time()\n",
        "lr.fit(X_source_aligned_dann, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = lr.predict(X_target_aligned_dann)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Logistic Regression DANN:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (DANN):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wbC01jNkKbp",
        "outputId": "131c9dee-b926-4e9c-c516-3330157b622d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression DANN:\n",
            "DANN:  Accuracy = 0.9116, F1 = 0.9114, Precision = 0.9113, Recall = 0.9116\n",
            "Training Time: 1.3412 seconds\n",
            "Testing Time: 0.2234 seconds\n",
            "Scoring Time: 0.2161 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.9016 seconds\n",
            "Testing Time Without Score Calulations: 0.0073 seconds\n",
            "Class-wise Metrics (DANN):\n",
            "Label 0: Precision = 0.8231, Recall = 0.8158, F1-score = 0.8195\n",
            "Label 1: Precision = 0.9401, Recall = 0.9428, F1-score = 0.9415\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[26613  6007]\n",
            " [ 5718 94266]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uYOXz0VkjjW",
        "outputId": "6e0a5ccf-4c55-44c1-8836-75de34bcd2fa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0924, F1 Δ = +0.1212, Precision Δ = +0.0929, Recall Δ = +0.0924\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0087\n",
            "  Recall Δ = +0.4724\n",
            "  F1-score Δ = +0.3364\n",
            "Label 1:\n",
            "  Precision Δ = +0.1203\n",
            "  Recall Δ = -0.0317\n",
            "  F1-score Δ = +0.0510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression ADDA"
      ],
      "metadata": {
        "id": "OUfMj8nJlJvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Logistic Regression ADDA\n",
        "# #############################################\n",
        "\n",
        "\n",
        "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
        "train_start_time = time.time()\n",
        "lr.fit(X_source_aligned_adda, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = lr.predict(X_target_aligned_adda)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Logistic Regression ADDA:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (ADDA):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTyep_m0lJNJ",
        "outputId": "65639656-367d-4ef4-bc0c-d82365b6d86d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression ADDA:\n",
            "ADDA:  Accuracy = 0.9259, F1 = 0.9268, Precision = 0.9286, Recall = 0.9259\n",
            "Training Time: 1.9820 seconds\n",
            "Testing Time: 0.3004 seconds\n",
            "Scoring Time: 0.2908 seconds\n",
            "Training Time Without Testing And Score Calulations: 1.3908 seconds\n",
            "Testing Time Without Score Calulations: 0.0096 seconds\n",
            "Class-wise Metrics (ADDA):\n",
            "Label 0: Precision = 0.8236, Recall = 0.8892, F1-score = 0.8551\n",
            "Label 1: Precision = 0.9629, Recall = 0.9379, F1-score = 0.9502\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[29005  3615]\n",
            " [ 6214 93770]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-LMxHC4mLhX",
        "outputId": "6bf0df7a-6d52-4427-97c5-7cf201ce44c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.1185, F1 Δ = +0.1482, Precision Δ = +0.1217, Recall Δ = +0.1185\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0315\n",
            "  Recall Δ = +0.5697\n",
            "  F1-score Δ = +0.3951\n",
            "Label 1:\n",
            "  Precision Δ = +0.1511\n",
            "  Recall Δ = -0.0287\n",
            "  F1-score Δ = +0.0677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "C_jne9rQGXKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Without Domain Adaptation"
      ],
      "metadata": {
        "id": "qmHarxMjGZVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Random Forest without domain adaptation\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_start_time = time.time()\n",
        "rf.fit(X_source, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_before = rf.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "train_time_before = time.time() - train_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "score_time_before = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"Normal Approach: Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal Approach):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():  # Check if it's a class label\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-GDhdb0FCsA",
        "outputId": "188c798e-1606-40ff-e863-3c871f3c45f0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "Normal Approach: Accuracy = 0.8798, F1 = 0.8745, Precision = 0.8763, Recall = 0.8798\n",
            "Training Time: 114.0900 seconds\n",
            "Testing Time: 1.2125 seconds\n",
            "Scoring Time: 0.1751 seconds\n",
            "Training Time Without Testing And Score Calulations: 112.7025 seconds\n",
            "Testing Time Without Score Calulations: 1.0374 seconds\n",
            "Class-wise Metrics (Normal Approach):\n",
            "Label 0: Precision = 0.8265, Recall = 0.6471, F1-score = 0.7259\n",
            "Label 1: Precision = 0.8925, Recall = 0.9557, F1-score = 0.9230\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[21109 11511]\n",
            " [ 4431 95553]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Wasserstein"
      ],
      "metadata": {
        "id": "GuwHJl5GGlTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Random Forest Wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_start_time = time.time()\n",
        "rf.fit(X_source_aligned_wasserstein, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = rf.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdcuryc_Gj7U",
        "outputId": "5160dfd1-f357-49b1-b110-61446b7a5298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "Wasserstein:  Accuracy = 0.9034, F1 = 0.9044, Precision = 0.9059, Recall = 0.9034\n",
            "Training Time: 67.4229 seconds\n",
            "Testing Time: 0.3820 seconds\n",
            "Scoring Time: 0.0370 seconds\n",
            "Training Time Without Testing And Score Calulations: 67.0038 seconds\n",
            "Testing Time Without Score Calulations: 0.3450 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.7854, Recall = 0.8355, F1-score = 0.8097\n",
            "Label 1: Precision = 0.9452, Recall = 0.9255, F1-score = 0.9353\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[27255  5365]\n",
            " [ 7445 92539]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g06RX8MaHeO0",
        "outputId": "e667ec16-95bd-4379-9f2a-356e7469289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0236, F1 Δ = +0.0299, Precision Δ = +0.0296, Recall Δ = +0.0236\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0411\n",
            "  Recall Δ = +0.1884\n",
            "  F1-score Δ = +0.0838\n",
            "Label 1:\n",
            "  Precision Δ = +0.0527\n",
            "  Recall Δ = -0.0301\n",
            "  F1-score Δ = +0.0123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest classwise Wasserstein"
      ],
      "metadata": {
        "id": "tI5IOmI5pgVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Random Forest Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_start_time = time.time()\n",
        "rf.fit(X_source_aligned_wasserstein_classwise, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = rf.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"Classwise Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Classwise Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Classwise Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvOIzYevplmH",
        "outputId": "e3b7a5ae-a332-48dc-9670-e471174ef239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "Classwise Wasserstein:  Accuracy = 0.9250, F1 = 0.9248, Precision = 0.9247, Recall = 0.9250\n",
            "Training Time: 13.6459 seconds\n",
            "Testing Time: 0.3524 seconds\n",
            "Scoring Time: 0.0758 seconds\n",
            "Training Time Without Testing And Score Calulations: 13.2177 seconds\n",
            "Testing Time Without Score Calulations: 0.2766 seconds\n",
            "Class-wise Metrics (Classwise Wasserstein):\n",
            "Label 0: Precision = 0.8512, Recall = 0.8423, F1-score = 0.8467\n",
            "Label 1: Precision = 0.9487, Recall = 0.9520, F1-score = 0.9503\n",
            "\n",
            "Confusion Matrix (Classwise Wasserstein):\n",
            "[[27476  5144]\n",
            " [ 4804 95180]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN1ab_w1p7y-",
        "outputId": "b62c0c4a-7b2a-46c2-cbc8-2e97acb8f5a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0452, F1 Δ = +0.0503, Precision Δ = +0.0485, Recall Δ = +0.0452\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0247\n",
            "  Recall Δ = +0.1952\n",
            "  F1-score Δ = +0.1208\n",
            "Label 1:\n",
            "  Precision Δ = +0.0562\n",
            "  Recall Δ = -0.0037\n",
            "  F1-score Δ = +0.0273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest DANN"
      ],
      "metadata": {
        "id": "dvjz6ULYGLMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Random Forest DANN\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_start_time = time.time()\n",
        "rf.fit(X_source_aligned_dann, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = rf.predict(X_target_aligned_dann)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (DANN):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K3d7C8FGKpY",
        "outputId": "ebf8a76b-e817-4aaf-8abb-74aaa7dc8d64"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "DANN:  Accuracy = 0.9254, F1 = 0.9267, Precision = 0.9294, Recall = 0.9254\n",
            "Training Time: 68.0802 seconds\n",
            "Testing Time: 1.1191 seconds\n",
            "Scoring Time: 0.1775 seconds\n",
            "Training Time Without Testing And Score Calulations: 66.7837 seconds\n",
            "Testing Time Without Score Calulations: 0.9416 seconds\n",
            "Class-wise Metrics (DANN):\n",
            "Label 0: Precision = 0.8146, Recall = 0.9021, F1-score = 0.8561\n",
            "Label 1: Precision = 0.9669, Recall = 0.9330, F1-score = 0.9497\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[29425  3195]\n",
            " [ 6695 93289]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1MjakG-GZSJ",
        "outputId": "fa0fcec1-4b3b-474f-9b41-67d65f70e14a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0456, F1 Δ = +0.0521, Precision Δ = +0.0532, Recall Δ = +0.0456\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0119\n",
            "  Recall Δ = +0.2549\n",
            "  F1-score Δ = +0.1302\n",
            "Label 1:\n",
            "  Precision Δ = +0.0744\n",
            "  Recall Δ = -0.0226\n",
            "  F1-score Δ = +0.0267\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest ADDA"
      ],
      "metadata": {
        "id": "ZzPxaY40GdSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Random Forest ADDA\n",
        "# #############################################\n",
        "\n",
        "\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "train_start_time = time.time()\n",
        "rf.fit(X_source_aligned_adda, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = rf.predict(X_target_aligned_adda)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Random Forest Classifier:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (ADDA):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM7DuFgCGguZ",
        "outputId": "919d9bc1-0e6d-4222-adc4-9af57220b13f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier:\n",
            "ADDA:  Accuracy = 0.9291, F1 = 0.9302, Precision = 0.9326, Recall = 0.9291\n",
            "Training Time: 103.2255 seconds\n",
            "Testing Time: 1.0360 seconds\n",
            "Scoring Time: 0.1749 seconds\n",
            "Training Time Without Testing And Score Calulations: 102.0146 seconds\n",
            "Testing Time Without Score Calulations: 0.8612 seconds\n",
            "Class-wise Metrics (ADDA):\n",
            "Label 0: Precision = 0.8236, Recall = 0.9058, F1-score = 0.8627\n",
            "Label 1: Precision = 0.9682, Recall = 0.9367, F1-score = 0.9522\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[29546  3074]\n",
            " [ 6328 93656]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXwxQpp9GgOE",
        "outputId": "cecc08ef-defb-4b72-ed3e-f7e6ce809f60"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0493, F1 Δ = +0.0557, Precision Δ = +0.0564, Recall Δ = +0.0493\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0029\n",
            "  Recall Δ = +0.2586\n",
            "  F1-score Δ = +0.1368\n",
            "Label 1:\n",
            "  Precision Δ = +0.0757\n",
            "  Recall Δ = -0.0190\n",
            "  F1-score Δ = +0.0292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron"
      ],
      "metadata": {
        "id": "OHFSr1KQtHog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier Without Domain Adaptation"
      ],
      "metadata": {
        "id": "7fn8xRVTtO-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Neural Network without domain adaptation\n",
        "# #############################################\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "mlp.fit(X_source, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_before = mlp.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "train_time_before = time.time() - train_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "score_time_before = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(f\"Normal:  Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfeUZKpEtOeo",
        "outputId": "ef1b39da-8cc3-4b0f-d626-b15085e8f3f1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network:\n",
            "Normal:  Accuracy = 0.8089, F1 = 0.7792, Precision = 0.8009, Recall = 0.8089\n",
            "Training Time: 509.4440 seconds\n",
            "Testing Time: 1.0048 seconds\n",
            "Scoring Time: 0.3047 seconds\n",
            "Training Time Without Testing And Score Calulations: 508.1344 seconds\n",
            "Testing Time Without Score Calulations: 0.7001 seconds\n",
            "Class-wise Metrics (Normal):\n",
            "Label 0: Precision = 0.7579, Recall = 0.3281, F1-score = 0.4579\n",
            "Label 1: Precision = 0.8150, Recall = 0.9658, F1-score = 0.8840\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[10701 21919]\n",
            " [ 3419 96565]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier Wasserstein"
      ],
      "metadata": {
        "id": "EKAIQ8HCtqsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Neural Network wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "mlp.fit(X_source_aligned_wasserstein, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = mlp.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(f\"Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEV_1RS8tNmt",
        "outputId": "875cc83c-5b81-4dd9-b847-992d44536561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network:\n",
            "Wasserstein:  Accuracy = 0.8771, F1 = 0.8708, Precision = 0.8737, Recall = 0.8771\n",
            "Training Time: 527.3795 seconds\n",
            "Testing Time: 1.9282 seconds\n",
            "Scoring Time: 0.0964 seconds\n",
            "Training Time Without Testing And Score Calulations: 525.3549 seconds\n",
            "Testing Time Without Score Calulations: 1.8318 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8323, Recall = 0.6266, F1-score = 0.7149\n",
            "Label 1: Precision = 0.8873, Recall = 0.9588, F1-score = 0.9217\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[20440 12180]\n",
            " [ 4119 95865]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWADRl4dtu2q",
        "outputId": "3280e881-9a62-4f4f-b162-f105436f4312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0641, F1 Δ = +0.0871, Precision Δ = +0.0663, Recall Δ = +0.0641\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0544\n",
            "  Recall Δ = +0.2913\n",
            "  F1-score Δ = +0.2463\n",
            "Label 1:\n",
            "  Precision Δ = +0.0702\n",
            "  Recall Δ = -0.0100\n",
            "  F1-score Δ = +0.0352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier Calsswise Wasserstein"
      ],
      "metadata": {
        "id": "1n9XXmv-tytT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Neural Network Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "mlp.fit(X_source_aligned_wasserstein_classwise, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = mlp.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(f\"Calsswise Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Calsswise Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Calsswise Wasserstein):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgs-bIlqtyCb",
        "outputId": "2693bd7f-d16a-4379-caed-a3f79fce070f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network:\n",
            "Calsswise Wasserstein:  Accuracy = 0.9173, F1 = 0.9165, Precision = 0.9161, Recall = 0.9173\n",
            "Training Time: 36.9762 seconds\n",
            "Testing Time: 0.2773 seconds\n",
            "Scoring Time: 0.0922 seconds\n",
            "Training Time Without Testing And Score Calulations: 36.6066 seconds\n",
            "Testing Time Without Score Calulations: 0.1851 seconds\n",
            "Class-wise Metrics (Calsswise Wasserstein):\n",
            "Label 0: Precision = 0.8528, Recall = 0.8025, F1-score = 0.8269\n",
            "Label 1: Precision = 0.9368, Recall = 0.9548, F1-score = 0.9457\n",
            "\n",
            "Confusion Matrix (Calsswise Wasserstein):\n",
            "[[26179  6441]\n",
            " [ 4520 95464]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueX1-4UDt5_w",
        "outputId": "08935c76-2c9d-4823-8888-aa381d88c38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.1044, F1 Δ = +0.1328, Precision Δ = +0.1087, Recall Δ = +0.1044\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0749\n",
            "  Recall Δ = +0.4672\n",
            "  F1-score Δ = +0.3582\n",
            "Label 1:\n",
            "  Precision Δ = +0.1197\n",
            "  Recall Δ = -0.0140\n",
            "  F1-score Δ = +0.0592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPClassifier DANN"
      ],
      "metadata": {
        "id": "uQ0Qlp-jLhH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Neural Network Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "mlp = MLPClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "mlp.fit(X_source_aligned_dann, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = mlp.predict(X_target_aligned_dann)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (DANN):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "id": "ojgel-JVLgo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194954fc-a373-4799-d75c-243738018d8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network:\n",
            "DANN:  Accuracy = 0.9204, F1 = 0.9210, Precision = 0.9217, Recall = 0.9204\n",
            "Training Time: 92.7779 seconds\n",
            "Testing Time: 0.2108 seconds\n",
            "Scoring Time: 0.1573 seconds\n",
            "Training Time Without Testing And Score Calulations: 92.4098 seconds\n",
            "Testing Time Without Score Calulations: 0.0536 seconds\n",
            "Class-wise Metrics (DANN):\n",
            "Label 0: Precision = 0.8248, Recall = 0.8590, F1-score = 0.8416\n",
            "Label 1: Precision = 0.9534, Recall = 0.9405, F1-score = 0.9469\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[28022  4598]\n",
            " [ 5954 94030]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyYSEVYN-ZrI",
        "outputId": "747cf855-a2eb-416c-95c2-917aaff64d13"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.1346, F1 Δ = +0.1657, Precision Δ = +0.1492, Recall Δ = +0.1346\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0719\n",
            "  Recall Δ = +0.6414\n",
            "  F1-score Δ = +0.4363\n",
            "Label 1:\n",
            "  Precision Δ = +0.1744\n",
            "  Recall Δ = -0.0307\n",
            "  F1-score Δ = +0.0775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLPCLassifier ADDA"
      ],
      "metadata": {
        "id": "2XuyGXmi5h5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "mlp.fit(X_source_aligned_adda, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = mlp.predict(X_target_aligned_adda)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Neural Network:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (ADDA):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_QNSYIp5hE1",
        "outputId": "044c50b4-7173-48ed-c3e5-62fe6d2c5b4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Network:\n",
            "ADDA:  Accuracy = 0.9216, F1 = 0.9222, Precision = 0.9231, Recall = 0.9216\n",
            "Training Time: 184.7702 seconds\n",
            "Testing Time: 0.3167 seconds\n",
            "Scoring Time: 0.2275 seconds\n",
            "Training Time Without Testing And Score Calulations: 184.2260 seconds\n",
            "Testing Time Without Score Calulations: 0.0892 seconds\n",
            "Class-wise Metrics (ADDA):\n",
            "Label 0: Precision = 0.8254, Recall = 0.8642, F1-score = 0.8444\n",
            "Label 1: Precision = 0.9550, Recall = 0.9404, F1-score = 0.9476\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[28191  4429]\n",
            " [ 5963 94021]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK0muuQY-WXQ",
        "outputId": "1944b820-b845-47f8-fb3f-e1a224c43ade"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.1127, F1 Δ = +0.1430, Precision Δ = +0.1222, Recall Δ = +0.1127\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0675\n",
            "  Recall Δ = +0.5362\n",
            "  F1-score Δ = +0.3865\n",
            "Label 1:\n",
            "  Precision Δ = +0.1400\n",
            "  Recall Δ = -0.0254\n",
            "  F1-score Δ = +0.0636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "GLszGvgqt-dI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Without Domain Adaptation"
      ],
      "metadata": {
        "id": "iRfwCMx0ujTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Decision Tree without domain adaptation\n",
        "# #############################################\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "dt.fit(X_source, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_before = dt.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "train_time_before = time.time() - train_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "score_time_before = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(f\"Normal:  Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpMKYfFxuiMj",
        "outputId": "5d99504b-bb80-4e21-933a-74925e24d87f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "Normal:  Accuracy = 0.9174, F1 = 0.9174, Precision = 0.9174, Recall = 0.9174\n",
            "Training Time: 12.0704 seconds\n",
            "Testing Time: 0.1931 seconds\n",
            "Scoring Time: 0.1592 seconds\n",
            "Training Time Without Testing And Score Calulations: 11.7181 seconds\n",
            "Testing Time Without Score Calulations: 0.0339 seconds\n",
            "Class-wise Metrics (Normal):\n",
            "Label 0: Precision = 0.8314, Recall = 0.8330, F1-score = 0.8322\n",
            "Label 1: Precision = 0.9455, Recall = 0.9449, F1-score = 0.9452\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[27173  5447]\n",
            " [ 5511 94473]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Wasserstein"
      ],
      "metadata": {
        "id": "jCtAMfoouvBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Decision Tree Wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "dt.fit(X_source_aligned_wasserstein, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = dt.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(f\"Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dH3_oC3gut4i",
        "outputId": "00da68bc-5734-4f13-9d02-bfa5a19be728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "Wasserstein:  Accuracy = 0.1269, F1 = 0.0880, Precision = 0.1432, Recall = 0.1269\n",
            "Training Time: 12.7743 seconds\n",
            "Testing Time: 0.0945 seconds\n",
            "Scoring Time: 0.0650 seconds\n",
            "Training Time Without Testing And Score Calulations: 12.6149 seconds\n",
            "Testing Time Without Score Calulations: 0.0295 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.1221, Recall = 0.4118, F1-score = 0.1883\n",
            "Label 1: Precision = 0.1501, Recall = 0.0339, F1-score = 0.0553\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[13434 19186]\n",
            " [96596  3388]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8TOxfSou3HJ",
        "outputId": "13e0c0e2-273a-451a-f25e-f685f4e0ded9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.7905, F1 Δ = -0.8294, Precision Δ = -0.7742, Recall Δ = -0.7905\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.7093\n",
            "  Recall Δ = -0.4212\n",
            "  F1-score Δ = -0.6439\n",
            "Label 1:\n",
            "  Precision Δ = -0.7954\n",
            "  Recall Δ = -0.9110\n",
            "  F1-score Δ = -0.8899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree Classwise Wasserstein"
      ],
      "metadata": {
        "id": "oPxpOpecvA9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# Decision Tree Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "dt.fit(X_source_aligned_wasserstein_classwise, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = dt.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(f\"Wasserstein Classwise:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein Classwise):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein Classwise):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBxZcT-TvAHt",
        "outputId": "694b1115-24e4-4e09-9bb1-fffd3d524eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "Wasserstein Classwise:  Accuracy = 0.2473, F1 = 0.0999, Precision = 0.7672, Recall = 0.2473\n",
            "Training Time: 0.9155 seconds\n",
            "Testing Time: 0.0811 seconds\n",
            "Scoring Time: 0.0628 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.7715 seconds\n",
            "Testing Time Without Score Calulations: 0.0183 seconds\n",
            "Class-wise Metrics (Wasserstein Classwise):\n",
            "Label 0: Precision = 0.2463, Recall = 0.9996, F1-score = 0.3952\n",
            "Label 1: Precision = 0.9372, Recall = 0.0018, F1-score = 0.0036\n",
            "\n",
            "Confusion Matrix (Wasserstein Classwise):\n",
            "[[32608    12]\n",
            " [99805   179]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWVECBU8vIyR",
        "outputId": "68c04caf-243e-4f4a-8683-13e93ddda5e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.6701, F1 Δ = -0.8175, Precision Δ = -0.1502, Recall Δ = -0.6701\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.5851\n",
            "  Recall Δ = +0.1666\n",
            "  F1-score Δ = -0.4370\n",
            "Label 1:\n",
            "  Precision Δ = -0.0083\n",
            "  Recall Δ = -0.9431\n",
            "  F1-score Δ = -0.9416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree DANN"
      ],
      "metadata": {
        "id": "lPAhYYCs0OIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "dt.fit(X_source_aligned_dann, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = dt.predict(X_target_aligned_dann)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (DANN):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZidw95-0Tom",
        "outputId": "343e31e0-c66e-460b-d93b-6c42e8200c55"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "DANN:  Accuracy = 0.8877, F1 = 0.8848, Precision = 0.8846, Recall = 0.8877\n",
            "Training Time: 3.0473 seconds\n",
            "Testing Time: 0.1783 seconds\n",
            "Scoring Time: 0.1663 seconds\n",
            "Training Time Without Testing And Score Calulations: 2.7027 seconds\n",
            "Testing Time Without Score Calulations: 0.0120 seconds\n",
            "Class-wise Metrics (DANN):\n",
            "Label 0: Precision = 0.8145, Recall = 0.7038, F1-score = 0.7551\n",
            "Label 1: Precision = 0.9075, Recall = 0.9477, F1-score = 0.9271\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[22957  9663]\n",
            " [ 5230 94754]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhoHJtK50vMH",
        "outputId": "19624667-2303-4481-d4a8-119764ca407e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.0297, F1 Δ = -0.0326, Precision Δ = -0.0328, Recall Δ = -0.0297\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0169\n",
            "  Recall Δ = -0.1292\n",
            "  F1-score Δ = -0.0771\n",
            "Label 1:\n",
            "  Precision Δ = -0.0380\n",
            "  Recall Δ = +0.0028\n",
            "  F1-score Δ = -0.0180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree ADDA"
      ],
      "metadata": {
        "id": "HbfrM3836Z-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "train_start_time = time.time()\n",
        "dt.fit(X_source_aligned_adda, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = dt.predict(X_target_aligned_adda)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"Decision Tree:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (ADDA):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Leu7Po6ckD",
        "outputId": "9bb69507-fba1-4e14-e319-9e91e7dec4d4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "ADDA:  Accuracy = 0.2301, F1 = 0.2446, Precision = 0.3888, Recall = 0.2301\n",
            "Training Time: 5.3323 seconds\n",
            "Testing Time: 0.1841 seconds\n",
            "Scoring Time: 0.1726 seconds\n",
            "Training Time Without Testing And Score Calulations: 4.9756 seconds\n",
            "Testing Time Without Score Calulations: 0.0115 seconds\n",
            "Class-wise Metrics (ADDA):\n",
            "Label 0: Precision = 0.1331, Recall = 0.3863, F1-score = 0.1980\n",
            "Label 1: Precision = 0.4723, Recall = 0.1792, F1-score = 0.2598\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[12600 20020]\n",
            " [82069 17915]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyq3meMT-K8-",
        "outputId": "2b8e65c1-aa1a-410d-8ebf-2c5c87b9d881"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.6872, F1 Δ = -0.6728, Precision Δ = -0.5286, Recall Δ = -0.6872\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.6983\n",
            "  Recall Δ = -0.4468\n",
            "  F1-score Δ = -0.6342\n",
            "Label 1:\n",
            "  Precision Δ = -0.4732\n",
            "  Recall Δ = -0.7657\n",
            "  F1-score Δ = -0.6854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "uSLa9xkavTzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Without Domain Adaptation"
      ],
      "metadata": {
        "id": "hIr90sKgvfPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# K-Nearest Neighbors without domain adaptation\n",
        "# #############################################\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "train_start_time = time.time()\n",
        "knn.fit(X_source, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_before = knn.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "train_time_before = time.time() - train_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "score_time_before = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"K-Nearest Neighbors:\")\n",
        "print(f\"Normal:  Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lNwN0QKvSx8",
        "outputId": "09bc7aae-f72a-4e28-a483-d3cb87dd15c0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors:\n",
            "Normal:  Accuracy = 0.8458, F1 = 0.8325, Precision = 0.8407, Recall = 0.8458\n",
            "Training Time: 347.4489 seconds\n",
            "Testing Time: 347.2962 seconds\n",
            "Scoring Time: 0.1646 seconds\n",
            "Training Time Without Testing And Score Calulations: -0.0119 seconds\n",
            "Testing Time Without Score Calulations: 347.1317 seconds\n",
            "Class-wise Metrics (Normal):\n",
            "Label 0: Precision = 0.7985, Recall = 0.4993, F1-score = 0.6144\n",
            "Label 1: Precision = 0.8544, Recall = 0.9589, F1-score = 0.9037\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[16286 16334]\n",
            " [ 4110 95874]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Wasserstein"
      ],
      "metadata": {
        "id": "IC3SeKc2vqpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# K-Nearest Neighbors Wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "train_start_time = time.time()\n",
        "knn.fit(X_source_aligned_wasserstein, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = knn.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"K-Nearest Neighbors:\")\n",
        "print(f\"Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiK8TsLNvqHT",
        "outputId": "0aded3f4-2739-482a-d4aa-e6d9f4fc612f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors:\n",
            "Wasserstein:  Accuracy = 0.9142, F1 = 0.9144, Precision = 0.9147, Recall = 0.9142\n",
            "Training Time: 345.7367 seconds\n",
            "Testing Time: 345.6330 seconds\n",
            "Scoring Time: 0.0726 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.0312 seconds\n",
            "Testing Time Without Score Calulations: 345.5604 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8198, Recall = 0.8346, F1-score = 0.8271\n",
            "Label 1: Precision = 0.9457, Recall = 0.9401, F1-score = 0.9429\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[27224  5396]\n",
            " [ 5985 93999]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd0dvf4cv2B5",
        "outputId": "73de2432-15d4-424f-d4e6-3a2a5c7bab1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0683, F1 Δ = +0.0819, Precision Δ = +0.0741, Recall Δ = +0.0683\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0213\n",
            "  Recall Δ = +0.3353\n",
            "  F1-score Δ = +0.2127\n",
            "Label 1:\n",
            "  Precision Δ = +0.0913\n",
            "  Recall Δ = -0.0188\n",
            "  F1-score Δ = +0.0393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Classwise Wasserstein"
      ],
      "metadata": {
        "id": "x9lhcx7Bv79t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# K-Nearest Neighbors Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "train_start_time = time.time()\n",
        "knn.fit(X_source_aligned_wasserstein_classwise, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = knn.predict(X_target)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"K-Nearest Neighbors:\")\n",
        "print(f\"Wasserstein Classwise:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein Classwise):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein Classwise):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmvsMBzjv5O5",
        "outputId": "5a026cd0-d81b-42ad-b58c-0c494ff8611a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K-Nearest Neighbors:\n",
            "Wasserstein Classwise:  Accuracy = 0.8150, F1 = 0.7757, Precision = 0.8303, Recall = 0.8150\n",
            "Training Time: 348.1721 seconds\n",
            "Testing Time: 348.0683 seconds\n",
            "Scoring Time: 0.0807 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.0231 seconds\n",
            "Testing Time Without Score Calulations: 347.9876 seconds\n",
            "Class-wise Metrics (Wasserstein Classwise):\n",
            "Label 0: Precision = 0.8983, Recall = 0.2795, F1-score = 0.4263\n",
            "Label 1: Precision = 0.8081, Recall = 0.9897, F1-score = 0.8897\n",
            "\n",
            "Confusion Matrix (Wasserstein Classwise):\n",
            "[[ 9117 23503]\n",
            " [ 1032 98952]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq9fzrLowFHG",
        "outputId": "efa263de-6931-4425-ef7c-607e04cf3c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.0309, F1 Δ = -0.0568, Precision Δ = -0.0104, Recall Δ = -0.0309\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0998\n",
            "  Recall Δ = -0.2198\n",
            "  F1-score Δ = -0.1880\n",
            "Label 1:\n",
            "  Precision Δ = -0.0464\n",
            "  Recall Δ = +0.0308\n",
            "  F1-score Δ = -0.0140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN DANN"
      ],
      "metadata": {
        "id": "ZSTF4ZAJ1Gma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier()\n",
        "train_start_time = time.time()\n",
        "knn.fit(X_source_aligned_dann, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = knn.predict(X_target_aligned_dann)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"KNN:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (DANN):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtBDlmZ71Jm6",
        "outputId": "f912b0d7-8470-4404-ca9d-162d624f6e3a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN:\n",
            "DANN:  Accuracy = 0.9253, F1 = 0.9260, Precision = 0.9271, Recall = 0.9253\n",
            "Training Time: 14.4303 seconds\n",
            "Testing Time: 13.8131 seconds\n",
            "Scoring Time: 0.1658 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.4514 seconds\n",
            "Testing Time Without Score Calulations: 13.6473 seconds\n",
            "Class-wise Metrics (DANN):\n",
            "Label 0: Precision = 0.8289, Recall = 0.8773, F1-score = 0.8524\n",
            "Label 1: Precision = 0.9592, Recall = 0.9409, F1-score = 0.9500\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[28618  4002]\n",
            " [ 5909 94075]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "id": "gkwr1iCk1PTz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39a55570-be8f-4661-d78f-3b84e04929d4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0794, F1 Δ = +0.0935, Precision Δ = +0.0865, Recall Δ = +0.0794\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0304\n",
            "  Recall Δ = +0.3781\n",
            "  F1-score Δ = +0.2380\n",
            "Label 1:\n",
            "  Precision Δ = +0.1048\n",
            "  Recall Δ = -0.0180\n",
            "  F1-score Δ = +0.0463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN ADDA"
      ],
      "metadata": {
        "id": "1Wjy91fY6s3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier()\n",
        "train_start_time = time.time()\n",
        "knn.fit(X_source_aligned_adda, y_source)\n",
        "test_start_time = time.time()\n",
        "pred_after = knn.predict(X_target_aligned_adda)\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "score_time_after = time.time() - score_start_time\n",
        "\n",
        "\n",
        "print(\"KNN:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (ADDA):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\") # Commenting out since the original code didn't print confusion matrix\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8sskctu6s3m",
        "outputId": "cffb29e5-0518-4144-e156-a14475f7c54a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN:\n",
            "ADDA:  Accuracy = 0.9157, F1 = 0.9157, Precision = 0.9157, Recall = 0.9157\n",
            "Training Time: 21.0980 seconds\n",
            "Testing Time: 20.4766 seconds\n",
            "Scoring Time: 0.1640 seconds\n",
            "Training Time Without Testing And Score Calulations: 0.4574 seconds\n",
            "Testing Time Without Score Calulations: 20.3126 seconds\n",
            "Class-wise Metrics (ADDA):\n",
            "Label 0: Precision = 0.8296, Recall = 0.8273, F1-score = 0.8285\n",
            "Label 1: Precision = 0.9437, Recall = 0.9446, F1-score = 0.9441\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[26988  5632]\n",
            " [ 5542 94442]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIoog2wS-GW3",
        "outputId": "3dfcf7f2-f7ad-4e86-ea36-f99aa89ab9ee"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0699, F1 Δ = +0.0832, Precision Δ = +0.0750, Recall Δ = +0.0699\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0311\n",
            "  Recall Δ = +0.3281\n",
            "  F1-score Δ = +0.2141\n",
            "Label 1:\n",
            "  Precision Δ = +0.0893\n",
            "  Recall Δ = -0.0143\n",
            "  F1-score Δ = +0.0405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression With Particle Swarm Optimization"
      ],
      "metadata": {
        "id": "YKobt7LczuNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR + PSO Without Domain Adaptation"
      ],
      "metadata": {
        "id": "RjFripu6z01B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sigmoid function\n",
        "def sigmoid(z):\n",
        "    # z = np.clip(z, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + expit(-z))\n",
        "\n",
        "# Define objective function to minimize: 1 - accuracy\n",
        "def objective_function(weights):\n",
        "    z = np.dot(X_source, weights)\n",
        "    predictions = sigmoid(z) > 0.5\n",
        "    acc = accuracy_score(y_source, predictions)\n",
        "    return 1 - acc  # We want to maximize accuracy, so we minimize (1 - accuracy)\n",
        "\n",
        "train_start_time = time.time()\n",
        "dim = X_source.shape[1]\n",
        "lb = [-20] * dim  # Lower bounds\n",
        "ub = [20] * dim   # Upper bounds\n",
        "best_weights, fopt = pso(objective_function, lb, ub, swarmsize=30, maxiter=500)\n",
        "\n",
        "\n",
        "test_start_time = time.time()\n",
        "z_test = np.dot(X_target, best_weights)\n",
        "pred_before = sigmoid(z_test) > 0.5\n",
        "\n",
        "\n",
        "score_start_time = time.time()\n",
        "acc_before = accuracy_score(y_target, pred_before)\n",
        "f1_before = f1_score(y_target, pred_before, average='weighted')  # Weighted average for multi-class\n",
        "precision_before = precision_score(y_target, pred_before, average='weighted')  # Overall precision\n",
        "recall_before = recall_score(y_target, pred_before, average='weighted')  # Overall recall\n",
        "report_before = classification_report(y_target, pred_before, output_dict=True)\n",
        "cm_before = confusion_matrix(y_target, pred_before)\n",
        "score_time_before = time.time() - score_start_time\n",
        "test_time_before = time.time() - test_start_time\n",
        "train_time_before = time.time() - train_start_time\n",
        "\n",
        "\n",
        "print(\"LR + PSO:\")\n",
        "print(f\"Normal Approach: Accuracy = {acc_before:.4f}, F1 = {f1_before:.4f}, Precision = {precision_before:.4f}, Recall = {recall_before:.4f}\")\n",
        "print(f\"Training Time: {train_time_before:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_before:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_before:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_before - test_time_before - score_time_before):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_before - score_time_before):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Normal Approach):\")\n",
        "for label in report_before:\n",
        "    if label.isdigit():  # Check if it's a class label\n",
        "        print(f\"Label {label}: Precision = {report_before[label]['precision']:.4f}, Recall = {report_before[label]['recall']:.4f}, F1-score = {report_before[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Normal Approach):\")\n",
        "print(cm_before)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw3sY7A2wIYi",
        "outputId": "2e0169a2-b4f5-4872-a1fe-46fc9cf2a071"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 500\n",
            "LR + PSO:\n",
            "Normal Approach: Accuracy = 0.8529, F1 = 0.8395, Precision = 0.8503, Recall = 0.8529\n",
            "Training Time: 694.6435 seconds\n",
            "Testing Time: 0.2305 seconds\n",
            "Scoring Time: 0.2185 seconds\n",
            "Training Time Without Testing And Score Calulations: 694.1945 seconds\n",
            "Testing Time Without Score Calulations: 0.0119 seconds\n",
            "Class-wise Metrics (Normal Approach):\n",
            "Label 0: Precision = 0.8296, Recall = 0.5061, F1-score = 0.6287\n",
            "Label 1: Precision = 0.8571, Recall = 0.9661, F1-score = 0.9083\n",
            "\n",
            "Confusion Matrix (Normal Approach):\n",
            "[[16510 16110]\n",
            " [ 3392 96592]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR + PSO Wasserstein"
      ],
      "metadata": {
        "id": "KysQKqdW1aaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# PSO Wasserstein\n",
        "# #############################################\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # z = np.clip(z, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + expit(-z))\n",
        "\n",
        "# Define objective function to minimize: 1 - accuracy\n",
        "def objective_function(weights):\n",
        "    z = np.dot(X_source_aligned_wasserstein, weights)\n",
        "    predictions = sigmoid(z) > 0.5\n",
        "    acc = accuracy_score(y_source, predictions)\n",
        "    return 1 - acc  # We want to maximize accuracy, so we minimize (1 - accuracy)\n",
        "\n",
        "\n",
        "train_start_time = time.time()\n",
        "dim = X_source_aligned_wasserstein.shape[1]\n",
        "lb = [-20] * dim  # Lower bounds\n",
        "ub = [20] * dim   # Upper bounds\n",
        "best_weights, fopt = pso(objective_function, lb, ub, swarmsize=30, maxiter=500)\n",
        "\n",
        "\n",
        "test_start_time = time.time()\n",
        "z_test = np.dot(X_target, best_weights)\n",
        "pred_after = sigmoid(z_test) > 0.5\n",
        "\n",
        "\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "score_time_after = time.time() - score_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "train_time_after = time.time() - train_start_time\n",
        "\n",
        "\n",
        "print(\"LR + PSO Wasserstein:\")\n",
        "print(f\" Wasserstein:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a-beD0v0QBC",
        "outputId": "28d4b61a-9af0-40c9-82f4-03148fb4efc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 500\n",
            "LR + PSO Wasserstein:\n",
            " Wasserstein:  Accuracy = 0.9266, F1 = 0.9274, Precision = 0.9289, Recall = 0.9266\n",
            "Training Time: 468.5440 seconds\n",
            "Testing Time: 0.2031 seconds\n",
            "Scoring Time: 0.1826 seconds\n",
            "Training Time Without Testing And Score Calulations: 468.1582 seconds\n",
            "Testing Time Without Score Calulations: 0.0205 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8279, Recall = 0.8856, F1-score = 0.8558\n",
            "Label 1: Precision = 0.9618, Recall = 0.9399, F1-score = 0.9507\n",
            "\n",
            "Confusion Matrix (Wasserstein):\n",
            "[[28889  3731]\n",
            " [ 6006 93978]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD-yGM8y1yWS",
        "outputId": "85298155-f1ac-43a5-a6f1-31033edf451b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0096, F1 Δ = +0.0103, Precision Δ = +0.0116, Recall Δ = +0.0096\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0006\n",
            "  Recall Δ = +0.0485\n",
            "  F1-score Δ = +0.0236\n",
            "Label 1:\n",
            "  Precision Δ = +0.0152\n",
            "  Recall Δ = -0.0030\n",
            "  F1-score Δ = +0.0059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR + PSO Classwise Wasserstein"
      ],
      "metadata": {
        "id": "VBZW8J5x18Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# PSO Wasserstein Classwise\n",
        "# #############################################\n",
        "\n",
        "def sigmoid(z):\n",
        "    # z = np.clip(z, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + expit(-z))\n",
        "\n",
        "# Define objective function to minimize: 1 - accuracy\n",
        "def objective_function(weights):\n",
        "    z = np.dot(X_source_aligned_wasserstein_classwise, weights)\n",
        "    predictions = sigmoid(z) > 0.5\n",
        "    acc = accuracy_score(y_source, predictions)\n",
        "    return 1 - acc  # We want to maximize accuracy, so we minimize (1 - accuracy)\n",
        "\n",
        "\n",
        "train_start_time = time.time()\n",
        "dim = X_source_aligned_wasserstein_classwise.shape[1]\n",
        "lb = [-20] * dim  # Lower bounds\n",
        "ub = [20] * dim   # Upper bounds\n",
        "best_weights, fopt = pso(objective_function, lb, ub, swarmsize=30, maxiter=500)\n",
        "\n",
        "test_start_time = time.time()\n",
        "z_test = np.dot(X_target, best_weights)\n",
        "pred_after = sigmoid(z_test) > 0.5\n",
        "\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "score_time_after = time.time() - score_start_time\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "# time_after = time.time() - train_start_time # Calculate overall time\n",
        "\n",
        "print(\"LR + PSO Classwise Wasserstein:\")\n",
        "print(f\" Wasserstein Classwise:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (Wasserstein Classwise):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLAZxqkP17vy",
        "outputId": "a5dd25d8-098a-4ff3-f236-7e4f4f6d3ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 500\n",
            "LR + PSO Classwise Wasserstein:\n",
            " Wasserstein Classwise:  Accuracy = 0.8302, F1 = 0.8061, Precision = 0.8305, Recall = 0.8302\n",
            "Training Time: 459.8356 seconds\n",
            "Testing Time: 0.1390 seconds\n",
            "Scoring Time: 0.1291 seconds\n",
            "Training Time Without Testing And Score Calulations: 459.5675 seconds\n",
            "Testing Time Without Score Calulations: 0.0099 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8324, Recall = 0.3879, F1-score = 0.5292\n",
            "Label 1: Precision = 0.8299, Recall = 0.9745, F1-score = 0.8964\n",
            "\n",
            "Confusion Matrix (Wasserstein Classwise):\n",
            "[[12653 19967]\n",
            " [ 2548 97436]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0br-orKW3CSJ",
        "outputId": "c6af2435-1206-4977-d09a-85d0c29e773a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = -0.0867, F1 Δ = -0.1110, Precision Δ = -0.0867, Recall Δ = -0.0867\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = +0.0051\n",
            "  Recall Δ = -0.4492\n",
            "  F1-score Δ = -0.3030\n",
            "Label 1:\n",
            "  Precision Δ = -0.1167\n",
            "  Recall Δ = +0.0315\n",
            "  F1-score Δ = -0.0484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR + PSO DANN"
      ],
      "metadata": {
        "id": "AxiArUNN1wBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# PSO DANN\n",
        "# #############################################\n",
        "\n",
        "def sigmoid(z):\n",
        "    # z = np.clip(z, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + expit(-z))\n",
        "\n",
        "# Define objective function to minimize: 1 - accuracy\n",
        "def objective_function(weights):\n",
        "    z = np.dot(X_source_aligned_dann, weights)\n",
        "    predictions = sigmoid(z) > 0.5\n",
        "    acc = accuracy_score(y_source, predictions)\n",
        "    return 1 - acc  # We want to maximize accuracy, so we minimize (1 - accuracy)\n",
        "\n",
        "\n",
        "train_start_time = time.time()\n",
        "dim = X_source_aligned_dann.shape[1]\n",
        "lb = [-20] * dim  # Lower bounds\n",
        "ub = [20] * dim   # Upper bounds\n",
        "best_weights, fopt = pso(objective_function, lb, ub, swarmsize=30, maxiter=500)\n",
        "\n",
        "test_start_time = time.time()\n",
        "z_test = np.dot(X_target_aligned_dann, best_weights)\n",
        "pred_after = sigmoid(z_test) > 0.5\n",
        "\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "score_time_after = time.time() - score_start_time\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "# time_after = time.time() - train_start_time # Calculate overall time\n",
        "\n",
        "print(\"LR + PSO DANN:\")\n",
        "print(f\"DANN:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (DANN):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "id": "mXcMtzk23Nok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b1925d-1ae7-4813-b028-34955f660959"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 500\n",
            "LR + PSO DANN:\n",
            "DANN:  Accuracy = 0.9259, F1 = 0.9271, Precision = 0.9296, Recall = 0.9259\n",
            "Training Time: 602.3002 seconds\n",
            "Testing Time: 0.2215 seconds\n",
            "Scoring Time: 0.2124 seconds\n",
            "Training Time Without Testing And Score Calulations: 601.8662 seconds\n",
            "Testing Time Without Score Calulations: 0.0091 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8174, Recall = 0.8998, F1-score = 0.8567\n",
            "Label 1: Precision = 0.9662, Recall = 0.9344, F1-score = 0.9501\n",
            "\n",
            "Confusion Matrix (DANN):\n",
            "[[29353  3267]\n",
            " [ 6556 93428]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "id": "4FxZo9so67Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015d7d0d-6b34-4eb6-966e-c51c4cdd5f5e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0730, F1 Δ = +0.0876, Precision Δ = +0.0793, Recall Δ = +0.0730\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0121\n",
            "  Recall Δ = +0.3937\n",
            "  F1-score Δ = +0.2280\n",
            "Label 1:\n",
            "  Precision Δ = +0.1092\n",
            "  Recall Δ = -0.0316\n",
            "  F1-score Δ = +0.0417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR + PSO ADDA"
      ],
      "metadata": {
        "id": "_bIVmn3I67wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################\n",
        "# PSO ADDA\n",
        "# #############################################\n",
        "\n",
        "def sigmoid(z):\n",
        "    # z = np.clip(z, -500, 500)  # Prevent overflow\n",
        "    return 1 / (1 + expit(-z))\n",
        "\n",
        "# Define objective function to minimize: 1 - accuracy\n",
        "def objective_function(weights):\n",
        "    z = np.dot(X_source_aligned_adda, weights)\n",
        "    predictions = sigmoid(z) > 0.5\n",
        "    acc = accuracy_score(y_source, predictions)\n",
        "    return 1 - acc  # We want to maximize accuracy, so we minimize (1 - accuracy)\n",
        "\n",
        "\n",
        "train_start_time = time.time()\n",
        "dim = X_source_aligned_adda.shape[1]\n",
        "lb = [-20] * dim  # Lower bounds\n",
        "ub = [20] * dim   # Upper bounds\n",
        "best_weights, fopt = pso(objective_function, lb, ub, swarmsize=30, maxiter=500)\n",
        "\n",
        "test_start_time = time.time()\n",
        "z_test = np.dot(X_target_aligned_adda, best_weights)\n",
        "pred_after = sigmoid(z_test) > 0.5\n",
        "\n",
        "score_start_time = time.time()\n",
        "acc_after = accuracy_score(y_target, pred_after)\n",
        "f1_after = f1_score(y_target, pred_after, average='weighted')  # Weighted average for multi-class\n",
        "precision_after = precision_score(y_target, pred_after, average='weighted')  # Overall precision\n",
        "recall_after = recall_score(y_target, pred_after, average='weighted')  # Overall recall\n",
        "report_after = classification_report(y_target, pred_after, output_dict=True)\n",
        "cm_after = confusion_matrix(y_target, pred_after)\n",
        "score_time_after = time.time() - score_start_time\n",
        "train_time_after = time.time() - train_start_time\n",
        "test_time_after = time.time() - test_start_time\n",
        "# time_after = time.time() - train_start_time # Calculate overall time\n",
        "\n",
        "print(\"LR + PSO ADDA:\")\n",
        "print(f\"ADDA:  Accuracy = {acc_after:.4f}, F1 = {f1_after:.4f}, Precision = {precision_after:.4f}, Recall = {recall_after:.4f}\")\n",
        "print(f\"Training Time: {train_time_after:.4f} seconds\")\n",
        "print(f\"Testing Time: {test_time_after:.4f} seconds\")\n",
        "print(f\"Scoring Time: {score_time_after:.4f} seconds\")\n",
        "print(f\"Training Time Without Testing And Score Calulations: {(train_time_after - test_time_after - score_time_after):.4f} seconds\")\n",
        "print(f\"Testing Time Without Score Calulations: {(test_time_after - score_time_after):.4f} seconds\")\n",
        "print(\"Class-wise Metrics (Wasserstein):\")\n",
        "for label in report_after:\n",
        "    if label.isdigit():\n",
        "        print(f\"Label {label}: Precision = {report_after[label]['precision']:.4f}, Recall = {report_after[label]['recall']:.4f}, F1-score = {report_after[label]['f1-score']:.4f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix (ADDA):\")\n",
        "print(cm_after)"
      ],
      "metadata": {
        "id": "W8F41GeC67wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d242fca6-3efb-499d-f542-0b6848869280"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping search: maximum iterations reached --> 500\n",
            "LR + PSO ADDA:\n",
            "ADDA:  Accuracy = 0.9424, F1 = 0.9440, Precision = 0.9503, Recall = 0.9424\n",
            "Training Time: 586.3792 seconds\n",
            "Testing Time: 0.2217 seconds\n",
            "Scoring Time: 0.2135 seconds\n",
            "Training Time Without Testing And Score Calulations: 585.9440 seconds\n",
            "Testing Time Without Score Calulations: 0.0081 seconds\n",
            "Class-wise Metrics (Wasserstein):\n",
            "Label 0: Precision = 0.8221, Recall = 0.9775, F1-score = 0.8931\n",
            "Label 1: Precision = 0.9922, Recall = 0.9310, F1-score = 0.9606\n",
            "\n",
            "Confusion Matrix (ADDA):\n",
            "[[31885   735]\n",
            " [ 6900 93084]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Improvement:  Accuracy Δ = {acc_after - acc_before:+.4f}, F1 Δ = {f1_after - f1_before:+.4f}, Precision Δ = {precision_after - precision_before:+.4f}, Recall Δ = {recall_after - recall_before:+.4f}\\n\")\n",
        "\n",
        "print(\"Class-wise Improvement:\")\n",
        "for label in report_before:\n",
        "    if label.isdigit() and label in report_after:\n",
        "        print(f\"Label {label}:\")\n",
        "        print(f\"  Precision Δ = {report_after[label]['precision'] - report_before[label]['precision']:+.4f}\")\n",
        "        print(f\"  Recall Δ = {report_after[label]['recall'] - report_before[label]['recall']:+.4f}\")\n",
        "        print(f\"  F1-score Δ = {report_after[label]['f1-score'] - report_before[label]['f1-score']:+.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICw0sbID-BwA",
        "outputId": "e0f6da73-0f42-4938-f01e-1b9003c37585"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improvement:  Accuracy Δ = +0.0895, F1 Δ = +0.1045, Precision Δ = +0.1000, Recall Δ = +0.0895\n",
            "\n",
            "Class-wise Improvement:\n",
            "Label 0:\n",
            "  Precision Δ = -0.0075\n",
            "  Recall Δ = +0.4713\n",
            "  F1-score Δ = +0.2644\n",
            "Label 1:\n",
            "  Precision Δ = +0.1351\n",
            "  Recall Δ = -0.0351\n",
            "  F1-score Δ = +0.0523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c6I_-9yx-Cm0"
      },
      "execution_count": 45,
      "outputs": []
    }
  ]
}